{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# BIFROST reduction of simulated data\n",
    "\n",
    "This notebook demonstrates the basic data reduction workflow for BIFROST.\n",
    "It uses data that was simulated with McStas and a dedicated workflow that can process McStas data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipp as sc\n",
    "import sciline\n",
    "import scippnexus as snx\n",
    "\n",
    "from ess import bifrost\n",
    "from ess.bifrost.data import (\n",
    "    simulated_elastic_incoherent_with_phonon,\n",
    "    tof_lookup_table_simulation\n",
    ")\n",
    "from ess.spectroscopy.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "BIFROST NeXus files store detector data in 45 separate NXdetector groups, one per detector triplet.\n",
    "For the time being, we need to specify the names of these NXdetector groups when creating the workflow.\n",
    "So load them from the input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with snx.File(simulated_elastic_incoherent_with_phonon()) as f:\n",
    "    detector_names = list(f['entry/instrument'][snx.NXdetector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Generally, we would use all triplets, but for this example, we only use the first two.\n",
    "This reduces the size of the data and the time to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_names = detector_names[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Next, construct the workflow which is a [sciline.Pipeline](https://scipp.github.io/sciline/generated/classes/sciline.Pipeline.html) and encodes the entire reduction procedure.\n",
    "We need to provide a couple of parameters so we can run the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = bifrost.BifrostSimulationWorkflow(detector_names)\n",
    "# Set the input file name:\n",
    "workflow[Filename[SampleRun]] = simulated_elastic_incoherent_with_phonon()\n",
    "# Set the lookup table for frame unwrapping:\n",
    "workflow[TimeOfFlightLookupTableFilename] = tof_lookup_table_simulation()\n",
    "# We need to read many objects from the file,\n",
    "# keeping it open improves performance: (optional)\n",
    "workflow[PreopenNeXusFile] = PreopenNeXusFile(True)\n",
    "# We drop uncertainties where they would otherwise lead to correlations:\n",
    "workflow[UncertaintyBroadcastMode] = UncertaintyBroadcastMode.drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Next, draw the workflow as a graph to inspect the steps it will take to reduce the data.\n",
    "Note the groups where entries are labeled with `triplet=x`. (These labels will be `dim_0=x` if you don't have [Pandas](https://pandas.pydata.org/) installed.)\n",
    "In this example, there are only two triplets but, as explained above, in a realistic case, there would be 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.visualize(EnergyQDetector[SampleRun], compact=True, graph_attr={\"rankdir\": \"LR\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We are ready to compute the reduced data.\n",
    "We use the naive scheduler of sciline because it tends to perform better for BIFROST than the [Dask](https://docs.dask.org/en/stable/index.html) scheduler.\n",
    "But this is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = sciline.scheduler.NaiveScheduler()\n",
    "data = workflow.compute(EnergyQDetector[SampleRun], scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The result contains coordinates for the sample table and detector rotation angles `a3` and `a4`, respectively.\n",
    "It also contains event coordinates for `energy_transfer` and two momentum transfers, one in the lab frame and one in the sample table frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "We can plot the counts as a function of energy transfer and $a_3$ by removing the unused dimensions.\n",
    "As expected, it is independent of $a_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data['a4', 0]\n",
    "    .bins.concat()\n",
    "    .hist(energy_transfer=sc.linspace('energy_transfer', -0.05, 0.05, 200, unit='meV'))\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We can also plot the counts as a function of the momentum transfer in the sample table frame $Q$.\n",
    "For this, we first need to create a 2D slice in $Q$ and $\\Delta E$.\n",
    "For simplicity, we use the x and z axes or $Q$ (see https://scipp.github.io/scippneutron/user-guide/coordinate-transformations.html for definitions).\n",
    "But we could use any other normalized, orthogonal vectors in the dot products.\n",
    "\n",
    "We can make a second plot using one of the projections and the energy transfer.\n",
    "\n",
    "The plots are a bit coarse because we only used two triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data['a4', 0].bins.concat().copy()\n",
    "x = sc.vector([1, 0, 0])\n",
    "z = sc.vector([0, 0, 1])\n",
    "d.bins.coords['Qx'] = sc.dot(x, d.bins.coords['sample_table_momentum_transfer'])\n",
    "d.bins.coords['Qz'] = sc.dot(z, d.bins.coords['sample_table_momentum_transfer'])\n",
    "d.hist(Qz=100, Qx=100).plot(norm='log') / d.hist(energy_transfer=100, Qz=100).plot(norm='log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
